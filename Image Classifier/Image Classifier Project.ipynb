{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are one of the major sources of data in the field of data science and AI. This field is making appropriate use of information that can be gathered through images by examining its features and details. We are trying to give you an exposure of how an end to end project is developed in this field. \n",
    "The idea behind this project is to build a deep learning-based Image Classification model on images that will be scraped from e-commerce portal. This is done to make the model more and more robust. \n",
    "This task is divided into two phases: Data Collection and Mode Building. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will start with google colab because there no issue with python libraries their dependencies and also its cloud base environment so we will not need a lot of configuration\n",
    "here we have opened the google colab file and mounted my google drive for accessing the dataset stored in imageclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2bvlSvUEB3v",
    "outputId": "e0f715f4-a7d6-41d1-993c-4091416f4cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. import libraries for dataset reading and CNN (convolutional neural network) model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "KDgnam-nalQa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating folder in google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have created folder in google drive with name \"imageclassification\",which contain two folder taining dataset and testing dataset.Each folder having images of saree,jeans and trouser which has been scrapped from Amazon web sites. \n",
    "Also have setup the path of training and testing directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "q_QJbUe3aprg"
   },
   "outputs": [],
   "source": [
    "base_dir = '/content/drive/MyDrive/imageclassification/Dataset1'\n",
    "train_dir = '/content/drive/MyDrive/imageclassification/Dataset1/traindata'\n",
    "train_saree_dir = '/content/drive/MyDrive/imageclassification/Dataset1/traindata/saree'\n",
    "train_jeans_dir = '/content/drive/MyDrive/imageclassification/Dataset1/traindata/jeans'\n",
    "train_Trouser_dir = '/content/drive/MyDrive/imageclassification/Dataset1/traindata/Trouser'\n",
    "test_dir = '/content/drive/MyDrive/imageclassification/Dataset1/testdata'\n",
    "test_saree_dir = '/content/drive/MyDrive/imageclassification/Dataset1/testdata/saree'\n",
    "test_jeans_dir = '/content/drive/MyDrive/imageclassification/Dataset1/testdata/jeans'\n",
    "test_trouser_dir = '/content/drive/MyDrive/imageclassification/Dataset1/testdata/trouser'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Checking the number of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are Checking the number of images in each folder for training and testing and storing in the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "h-zJR7nwcLDf"
   },
   "outputs": [],
   "source": [
    "num_saree_train = len(os.listdir(train_saree_dir))\n",
    "num_jeans_train=len(os.listdir(train_jeans_dir))\n",
    "num_trouser_train=len(os.listdir(train_Trouser_dir))\n",
    "num_trouser_test=len(os.listdir(test_trouser_dir))\n",
    "num_jeans_test=len(os.listdir(test_jeans_dir))\n",
    "num_saree_test=len(os.listdir(test_saree_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0Z51O8mcu-N",
    "outputId": "25dc8da8-afae-4b95-d0e4-7c4b9b59d21b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Saree Images 199\n",
      "Total Training jeans Images 199\n",
      "Total Training trouser Images 199\n",
      "--\n",
      "Total Test Saree Images 90\n",
      "Total Test jeans Images 90\n",
      "Total test trouser Images 90\n",
      "--\n",
      "Total Training Images 597\n",
      "--\n",
      "Total Testing Images 270\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Training Saree Images\",num_saree_train)\n",
    "print(\"Total Training jeans Images\",num_jeans_train)\n",
    "print(\"Total Training trouser Images\",num_trouser_train)\n",
    "print(\"--\")\n",
    "print(\"Total Test Saree Images\",num_saree_test)\n",
    "print(\"Total Test jeans Images\",num_jeans_test)\n",
    "print(\"Total test trouser Images\",num_trouser_test)\n",
    "print(\"--\")\n",
    "total_train = num_saree_train+num_jeans_train+num_trouser_train\n",
    "total_test = num_saree_test+num_jeans_test+num_trouser_test\n",
    "print(\"Total Training Images\",total_train)\n",
    "print(\"--\")\n",
    "print(\"Total Testing Images\",total_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Setting up Images size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have set the size(height, width) of images. This step mostly needs when dataset images have different sizes, it will speed up the training process. I used an image shape of (224,224) becuase VGG-16 algorithm accepts only this image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "-u98ST7rdxaS"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE  = 224\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Preprocessing of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we have preporcessed our data (train and test), which includes, rescaling and shuffling.\n",
    "also using the Image Data Generator to import the images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrhcodtNjQPd",
    "outputId": "f4743167-f14a-4892-96e8-399323f01077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 597 images belonging to 3 classes.\n",
      "Found 270 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_train = ImageDataGenerator(rescale = 1./255)\n",
    "train_data_gen = image_gen_train.flow_from_directory(batch_size = batch_size,\n",
    "directory = train_dir,\n",
    "shuffle= True,\n",
    "target_size = (IMG_SHAPE,IMG_SHAPE),\n",
    "class_mode = 'categorical')\n",
    "\n",
    "image_gen_test = ImageDataGenerator(rescale=1./255)\n",
    "test_data_gen = image_gen_test.flow_from_directory(batch_size=batch_size,\n",
    "directory=test_dir,\n",
    "target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Downloading VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we have downloaded VGG-16 weights, by including the top layer parameter as false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBHRGI_7kXgF",
    "outputId": "8ff39e20-fc35-436d-b9b6-dba0f3faaf84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model = tf.keras.applications.VGG16(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Freezing the training layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we have freezed the training layers of VGG-16. (because VGG-16, already trained on huge data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qMgbffvksw2",
    "outputId": "1ddbdb3d-740f-450a-b0f9-04418fc71b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "block1_conv1\n",
      "block1_conv2\n",
      "block1_pool\n",
      "block2_conv1\n",
      "block2_conv2\n",
      "block2_pool\n",
      "block3_conv1\n",
      "block3_conv2\n",
      "block3_conv3\n",
      "block3_pool\n",
      "block4_conv1\n",
      "block4_conv2\n",
      "block4_conv3\n",
      "block4_pool\n",
      "block5_conv1\n",
      "block5_conv2\n",
      "block5_conv3\n",
      "block5_pool\n"
     ]
    }
   ],
   "source": [
    "for layer in pre_trained_model.layers:\n",
    "  print(layer.name)\n",
    "layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Modifying the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we have modified the last layer for our classes as all layers of VGG-16 are frozen.We have added one max polling, one dense layer, one dropout, and one output with the last layer of VGG-16.\n",
    "Since the problem having multiclass so the last dense layer has choosen with 3 and activation with softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "79itEyqulAbD"
   },
   "outputs": [],
   "source": [
    "last_layer = pre_trained_model.get_layer('block5_pool')\n",
    "last_output = last_layer.output\n",
    "x = tf.keras.layers.GlobalMaxPooling2D()(last_output)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(3, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Merge layers with custom layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have merged the original VGG-16 layers, with our custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "aFWR38QblWdn"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(pre_trained_model.input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have compiling the model before starting training and since problem is multiclass have choosen categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "GXOL90jzlaiC"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Checking model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-dwmj4SlqOe",
    "outputId": "21cb0e3a-a29c-4328-81f6-d70d39ca7c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 512)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,978,883\n",
      "Trainable params: 14,978,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Traning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained the model on five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1-v8CqzlvEW",
    "outputId": "23f0961c-e564-4a59-fac6-6dc21794ee39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - 952s 52s/step - loss: 1.4168 - acc: 0.3239\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 931s 52s/step - loss: 1.0847 - acc: 0.3894\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 930s 52s/step - loss: 0.8098 - acc: 0.5628\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 929s 52s/step - loss: 1.0782 - acc: 0.4637\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 929s 52s/step - loss: 0.7115 - acc: 0.6389\n"
     ]
    }
   ],
   "source": [
    "vgg_classifier = model.fit(train_data_gen,\n",
    "steps_per_epoch=(total_train//batch_size),\n",
    "epochs = 5,\n",
    "batch_size = batch_size,\n",
    "verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Model testing on testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yjh1XxHXmRAx",
    "outputId": "9cf2d40d-05bd-43cc-bedf-c693c50aa848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 124s 14s/step - loss: 2.6072 - acc: 0.1074\n",
      "test_loss, test accuracy [2.6072354316711426, 0.10740740597248077]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data_gen,batch_size=batch_size)\n",
    "print(\"test_loss, test accuracy\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qloIwz4Km9y6"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"/content/drive/MyDrive/imageclassification/image_classification.json\", \"w\") as json_file:\n",
    "json_file.write(model_json)\n",
    "model.save(\"/content/drive/MyDrive/imageclassification/image_classification.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "model.save_weights(\"/content/drive/MyDrive/imageclassification/image_classification.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
